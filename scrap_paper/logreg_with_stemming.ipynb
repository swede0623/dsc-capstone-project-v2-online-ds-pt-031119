{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from nltk import word_tokenize, regexp_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['char_total'] = train['comment_text'].map(lambda x: len(x))\n",
    "train = train[train['char_total']<4000]\n",
    "train.drop(['char_total'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df = pd.read_csv(\"test_labels.csv\")\n",
    "holdout = holdout.merge(test_labels_df, on='id')\n",
    "holdout.drop(holdout[holdout['toxic']==-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower everything\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: x.lower())\n",
    "\n",
    "# remove '\\\\n'\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[user.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#remove https links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(https://.*?\\s)|(https://.*)\",'',str(x)))\n",
    "\n",
    "#remove email addresses \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'',str(x)))\n",
    "\n",
    "#remove WP:__\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"wp:\\w*\",'',str(x)))\n",
    "\n",
    "#remove user::__\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"user::\\w*\",'',str(x)))\n",
    "\n",
    "#remove all websites\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\",'',str(x)))\n",
    "\n",
    "#remove all these auto-generated messages\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"preceding unsigned comment added by\",'',str(x)))\n",
    "\n",
    "#remove all punctuation\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower everything\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: x.lower())\n",
    "\n",
    "# remove '\\\\n'\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[user.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#remove https links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(https://.*?\\s)|(https://.*)\",'',str(x)))\n",
    "\n",
    "#remove email addresses \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'',str(x)))\n",
    "\n",
    "#remove WP:__\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"wp:\\w*\",'',str(x)))\n",
    "\n",
    "#remove user::__\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"user::\\w*\",'',str(x)))\n",
    "\n",
    "#remove all websites\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\",'',str(x)))\n",
    "\n",
    "#remove all these auto-generated messages\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"preceding unsigned comment added by\",'',str(x)))\n",
    "\n",
    "#remove all punctuation\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_digits(comment):\n",
    "    result = ''.join([i for i in comment if not i.isdigit()])\n",
    "    return result\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_all_digits)\n",
    "holdout['comment_text'] = holdout['comment_text'].apply(remove_all_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_token_lists = [regexp_tokenize(x,pattern=r'\\w{2,}') for x in train['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# flat = [item for items in list_of_token_lists for item in items]\n",
    "\n",
    "# flat = [token for token in flat if len(token)<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = [len(token) for token in flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter = PorterStemmer()\n",
    "# all_stems = []\n",
    "# for token in flat:\n",
    "#     s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "list_of_stems_lists = []\n",
    "for tokens in list_of_token_lists:\n",
    "    stems = [porter.stem(token) for token in tokens]\n",
    "    list_of_stems_lists.append(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer('english')\n",
    "# list_of_stems_lists = [[snowball.stem(token) for token in token_list] for token_list in list_of_token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems_for_tfidf = list(map(' '.join, list_of_stems_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = stems_for_tfidf\n",
    "y_train = train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_token_lists_test = [regexp_tokenize(x,pattern=r'\\w{2,}') for x in holdout['comment_text']]\n",
    "list_of_stems_lists_test = [[porter.stem(token) for token in token_list] for token_list in list_of_token_lists_test]\n",
    "stems_for_tfidf_test = list(map(' '.join, list_of_stems_lists_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = stems_for_tfidf_test\n",
    "y_test = holdout.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word',\n",
    "                                  use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = word_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class toxic is -0.11775666308745669\n",
      "CV Accuracy score for class toxic is 0.9569986753295906\n",
      "CV ROC_AUC score 0.9610830361170792\n",
      "\n",
      "CV Log_loss score for class severe_toxic is -0.02733017904575455\n",
      "CV Accuracy score for class severe_toxic is 0.9907777707689396\n",
      "CV ROC_AUC score 0.9829600787518241\n",
      "\n",
      "CV Log_loss score for class obscene is -0.06334644081588212\n",
      "CV Accuracy score for class obscene is 0.9779852393868669\n",
      "CV ROC_AUC score 0.9752797641384098\n",
      "\n",
      "CV Log_loss score for class threat is -0.011814697516991676\n",
      "CV Accuracy score for class threat is 0.9971551125969847\n",
      "CV ROC_AUC score 0.9869410904048856\n",
      "\n",
      "CV Log_loss score for class insult is -0.08218120562130721\n",
      "CV Accuracy score for class insult is 0.9694568851321517\n",
      "CV ROC_AUC score 0.9668109552083073\n",
      "\n",
      "CV Log_loss score for class identity_hate is -0.02720609078843697\n",
      "CV Accuracy score for class identity_hate is 0.9919699741373872\n",
      "CV ROC_AUC score 0.979651148334995\n",
      "\n",
      "Total average CV Log_loss score is -0.054939212812638194\n",
      "Total average CV ROC_AUC score is 0.9754543454925836\n"
     ]
    }
   ],
   "source": [
    "class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
