{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize, pos_tag, regexp_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df = pd.read_csv(\"test_labels.csv\")\n",
    "holdout = holdout.merge(test_labels_df, on='id')\n",
    "holdout.drop(holdout[holdout['toxic']==-1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['char_total'] = train['comment_text'].map(lambda x: len(x))\n",
    "# train = train[train['char_total']<2000]\n",
    "# train.drop(['char_total'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"-Preceding unsigned comment added by\", ' '.join(train['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower everything\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: x.lower())\n",
    "\n",
    "# remove '\\\\n'\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#remove https links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(https://.*?\\s)|(https://.*)\",'',str(x)))\n",
    "\n",
    "#remove email addresses \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'',str(x)))\n",
    "\n",
    "#remove WP:__\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"wp:\\w*\",'',str(x)))\n",
    "\n",
    "#remove user::__\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"user::\\w*\",'',str(x)))\n",
    "\n",
    "#remove all websites\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\",'',str(x)))\n",
    "\n",
    "#remove all these auto-generated messages\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"preceding unsigned comment added by\",'',str(x)))\n",
    "\n",
    "#remove all punctuation\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: x.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '\\\\n'\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#remove https links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(https://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#remove email addresses \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'',str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_digits(comment):\n",
    "    result = ''.join([i for i in comment if not i.isdigit()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(remove_all_digits)\n",
    "holdout['comment_text'] = holdout['comment_text'].apply(remove_all_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(comment):\n",
    "    lems = [token.lemma_ for token in comment if (token.lemma_ != '-PRON-' and \n",
    "                                   token.is_punct==False and token.is_space==False \n",
    "                                          and not token.lemma_.startswith(\"'\"))]\n",
    "    return lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bunch_df = train.sample(50000, random_state=0)\n",
    "train_bunch = train_bunch_df['comment_text']\n",
    "train_bunch_nlp = map(nlp, train_bunch)\n",
    "train_bunch_nlp_lem = map(lemmatize, train_bunch_nlp)\n",
    "train_bunch_nlp_lem_joined = list(map(' '.join, train_bunch_nlp_lem))\n",
    "\n",
    "train_bunch_labels = train_bunch_df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" I haven\\'t paraphrased you at all, Gary.  You complained that preferring recent sources to those \"\"well over  years old\"\" is \"\"recentism\"\".  I pointed out that it is strongly encouraged by MEDRS, and for good reason. Again, if you take issue with that, then you need to raise it at the appropriate talk page.   \"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_bunch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have not paraphrase at all gary complain that prefer recent source to those well over year old be recentism point out that be strongly encourage by medrs and for good reason again if take issue with that then need to raise at the appropriate talk page'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bunch_nlp_lem_joined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_bunch_nlp_lem_joined\n",
    "y_train = train_bunch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bunch_df = holdout.sample(25000, random_state=0)\n",
    "test_bunch = test_bunch_df['comment_text']\n",
    "test_bunch_nlp = map(nlp, test_bunch)\n",
    "test_bunch_nlp_lem = map(lemmatize, test_bunch_nlp)\n",
    "test_bunch_nlp_lem_joined = list(map(' '.join, test_bunch_nlp_lem))\n",
    "\n",
    "test_bunch_labels = test_bunch_df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_bunch_nlp_lem_joined\n",
    "y_test = test_bunch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('English') + list(string.punctuation) + [\"''\", '\"\"', '...', '``']\n",
    "stopwords_list += ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october',\n",
    "                  'november', 'december']\n",
    "stopwords_list += ['jan', 'feb', 'mar', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(stop_words=stopwords_list, token_pattern=\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = word_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class toxic is -0.14428029957498625\n",
      "CV Accuracy score for class toxic is 0.9467799999574232\n",
      "CV ROC_AUC score 0.9554741303022611\n",
      "\n",
      "CV Log_loss score for class severe_toxic is -0.031905787691422834\n",
      "CV Accuracy score for class severe_toxic is 0.9899400019919918\n",
      "CV ROC_AUC score 0.9841883966424407\n",
      "\n",
      "CV Log_loss score for class obscene is -0.08091604622176785\n",
      "CV Accuracy score for class obscene is 0.9722799855775355\n",
      "CV ROC_AUC score 0.9722639683096521\n",
      "\n",
      "CV Log_loss score for class threat is -0.014471298970617717\n",
      "CV Accuracy score for class threat is 0.9971799999977439\n",
      "CV ROC_AUC score 0.9807425050975185\n",
      "\n",
      "CV Log_loss score for class insult is -0.09485052641216762\n",
      "CV Accuracy score for class insult is 0.9657399999725915\n",
      "CV ROC_AUC score 0.9650012870916409\n",
      "\n",
      "CV Log_loss score for class identity_hate is -0.03264027705141962\n",
      "CV Accuracy score for class identity_hate is 0.9913599963930159\n",
      "CV ROC_AUC score 0.9759565214101674\n",
      "\n",
      "Total average CV Log_loss score is -0.06651070598706364\n",
      "Total average CV ROC_AUC score is 0.9722711348089469\n"
     ]
    }
   ],
   "source": [
    "class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=3, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=3, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
